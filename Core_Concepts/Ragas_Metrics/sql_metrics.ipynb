{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6f3d4d",
   "metadata": {},
   "source": [
    "# SQL Query Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2029380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a74d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Files\\RAG_with_RAGAS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "groq_llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "evaluator_llm = LangchainLLMWrapper(groq_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f4140",
   "metadata": {},
   "source": [
    "## Execution based metrics\n",
    "\n",
    "- the resulting SQL is compared after executing the SQL query on the database and then comparing the response with the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0dabc",
   "metadata": {},
   "source": [
    "#### DataCompyScore\n",
    "\n",
    "- DataCompyScore metric uses DataCompy, a python library that compares two pandas DataFrames. \n",
    "- It provides a simple interface to compare two DataFrames and provides a detailed report of the differences. \n",
    "- In this metric the response is executed on the database and the resulting data is compared with the expected data, ie reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024ff7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import DataCompyScore\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "data1 = \"\"\"acct_id,dollar_amt,name,float_fld,date_fld\n",
    "10000001234,123.45,George Maharis,14530.1555,2017-01-01\n",
    "10000001235,0.45,Michael Bluth,1,2017-01-01\n",
    "10000001236,1345,George Bluth,,2017-01-01\n",
    "10000001237,123456,Bob Loblaw,345.12,2017-01-01\n",
    "10000001238,1.05,Lucille Bluth,,2017-01-01\n",
    "10000001238,1.05,Loose Seal Bluth,,2017-01-01\n",
    "\"\"\"\n",
    "\n",
    "data2 = \"\"\"acct_id,dollar_amt,name,float_fld\n",
    "10000001234,123.4,George Michael Bluth,14530.155\n",
    "10000001235,0.45,Michael Bluth,\n",
    "10000001236,1345,George Bluth,1\n",
    "10000001237,123456,Robert Loblaw,345.12\n",
    "10000001238,1.05,Loose Seal Bluth,111\n",
    "\"\"\"\n",
    "sample = SingleTurnSample(response=data1, reference=data2)\n",
    "scorer = DataCompyScore(mode=\"rows\", metric=\"recall\")\n",
    "await scorer.single_turn_ascore(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1554b2d",
   "metadata": {},
   "source": [
    "## Non Execution based metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc764d",
   "metadata": {},
   "source": [
    "#### SQL Query Semantic equivalence\n",
    "\n",
    "- LLMSQLEquivalence is a metric that can be used to evaluate the equivalence of response query with reference query. \n",
    "- The metric also needs database schema to be used when comparing queries, this is inputted in reference_contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacf0197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMSQLEquivalence\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"\"\"\n",
    "        SELECT p.product_name, SUM(oi.quantity) AS total_quantity\n",
    "        FROM order_items oi\n",
    "        JOIN products p ON oi.product_id = p.product_id\n",
    "        GROUP BY p.product_name;\n",
    "    \"\"\",\n",
    "    reference=\"\"\"\n",
    "        SELECT p.product_name, COUNT(oi.quantity) AS total_quantity\n",
    "        FROM order_items oi\n",
    "        JOIN products p ON oi.product_id = p.product_id\n",
    "        GROUP BY p.product_name;\n",
    "    \"\"\",\n",
    "    reference_contexts=[\n",
    "        \"\"\"\n",
    "        Table order_items:\n",
    "        - order_item_id: INT\n",
    "        - order_id: INT\n",
    "        - product_id: INT\n",
    "        - quantity: INT\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Table products:\n",
    "        - product_id: INT\n",
    "        - product_name: VARCHAR\n",
    "        - price: DECIMAL\n",
    "        \"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer = LLMSQLEquivalence(llm= evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e556f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Complete_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
