{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f8ce57",
   "metadata": {},
   "source": [
    "# Natural Language Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469f76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5228da57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Files\\RAG_with_RAGAS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "groq_llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "evaluator_llm = LangchainLLMWrapper(groq_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb7806",
   "metadata": {},
   "source": [
    "#Factual Correctness\n",
    "\n",
    " - It is a metric that compares and evaluates the factual accuracy of the generated response with the reference. \n",
    " - This metric is used to determine the extent to which the generated response aligns with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5beef6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.67)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "from ragas import SingleTurnSample\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris. I has a height of 1000ft.\"\n",
    ")\n",
    "\n",
    "scorer = FactualCorrectness(llm= evaluator_llm, mode=\"f1\")\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7763890",
   "metadata": {},
   "source": [
    "###Controlling the Number of Claims\n",
    "\n",
    "- Atomicity refers to how much a sentence is broken down into its smallest, meaningful components.\n",
    "- Coverage refers to how comprehensively the claims represent the information in the original sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a9f44",
   "metadata": {},
   "source": [
    "#Semantic similarity\n",
    "\n",
    "- The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth.\n",
    "- This evaluation utilizes a bi-encoder model to calculate the semantic similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdab9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8968347134901562"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import SemanticSimilarity\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris. It has a height of 1000ft.\"\n",
    ")\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OllamaEmbeddings(model=\"llama3.2:1b\"))\n",
    "scorer = SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(evaluator_embeddings))\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef8e40",
   "metadata": {},
   "source": [
    "# Traditional Non-LLM Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b5989",
   "metadata": {},
   "source": [
    "#Traditional NLP Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d19ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918918918918919"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non LLM String Similarity\n",
    "# This metric measures the similarity between the reference and the response using traditional string distance measures \n",
    "# such as Levenshtein, Hamming, and Jaro.\n",
    "\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics._string import NonLLMStringSimilarity, DistanceMeasure\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in India.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\"\n",
    ")\n",
    "\n",
    "scorer = NonLLMStringSimilarity()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d76d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918918918918919"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = NonLLMStringSimilarity(distance_measure= DistanceMeasure.HAMMING)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec7879",
   "metadata": {},
   "source": [
    "#BLEU Score\n",
    "\n",
    "- The BleuScore score is a metric used to evaluate the quality of response by comparing it with reference\n",
    "- It measures the similarity between the response and the reference based on n-gram precision and brevity penalty.\n",
    "- BLEU score was originally designed to evaluate machine translation systems, but it is also used in other natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a57533a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865478"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import BleuScore\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in India.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\"\n",
    ")\n",
    "\n",
    "scorer = BleuScore()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef445c38",
   "metadata": {},
   "source": [
    "#ROUGE Score\n",
    "\n",
    "- The RougeScore score is a set of metrics used to evaluate the quality of natural language generations. \n",
    "- It measures the overlap between the generated response and the reference text based on n-gram recall, precision, and F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5078e227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import RougeScore\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in India.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\"\n",
    ")\n",
    "\n",
    "scorer = RougeScore()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2f5fb",
   "metadata": {},
   "source": [
    "#Exact Match\n",
    "\n",
    "- The ExactMatch metric checks if the response is exactly the same as the reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df480d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import ExactMatch\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"India\",\n",
    "    reference=\"Paris\"\n",
    ")\n",
    "\n",
    "scorer = ExactMatch()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be24e7",
   "metadata": {},
   "source": [
    "#String Presence\n",
    "\n",
    "- The StringPresence metric checks if the response contains the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0e85d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import StringPresence\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Eiffel Tower is located in India.\",\n",
    "    reference=\"Eiffel Tower\"\n",
    ")\n",
    "scorer = StringPresence()\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5a27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Complete_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
