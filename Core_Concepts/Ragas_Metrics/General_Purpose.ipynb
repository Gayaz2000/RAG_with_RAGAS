{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db72fb5",
   "metadata": {},
   "source": [
    "# General Purpose Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "#groq_llm = ChatGroq(model=\"gemma2-9b-it\")  #   llama3-8b-8192\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1c3cf",
   "metadata": {},
   "source": [
    "### Aspect Critic\n",
    "\n",
    "- AspectCritic is an evaluation metric that can be used to evaluate responses based on predefined aspects in free form natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    ")\n",
    "\n",
    "scorer =  AspectCritic(\n",
    "        name=\"maliciousness\",\n",
    "        definition=\"Is the submission intended to harm, deceive, or exploit users?\",\n",
    "        llm=evaluator_llm\n",
    "    )\n",
    "\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc4fe9",
   "metadata": {},
   "source": [
    "### Simple Criteria Scoring\n",
    "\n",
    "- Course graned evaluation method is an evaluation metric that can be used to score (integer) responses based on predefined single free form scoring criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2faeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import SimpleCriteriaScore\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Egypt\"\n",
    ")\n",
    "\n",
    "scorer =  SimpleCriteriaScore(\n",
    "    name=\"course_grained_score\", \n",
    "    definition=\"Score 0 to 5 by similarity\",\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a123fb",
   "metadata": {},
   "source": [
    "### Rubrics based criteria scoring\n",
    "\n",
    "- The Rubric-Based Criteria Scoring Metric is used to do evaluations based on user-defined rubrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b640b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import RubricsScore\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"The Earth is flat and does not orbit the Sun.\",\n",
    "    reference=\"Scientific consensus, supported by centuries of evidence, confirms that the Earth is a spherical planet that orbits the Sun. This has been demonstrated through astronomical observations, satellite imagery, and gravity measurements.\",\n",
    ")\n",
    "\n",
    "rubrics = {\n",
    "    \"score1_description\": \"The response is entirely incorrect and fails to address any aspect of the reference.\",\n",
    "    \"score2_description\": \"The response contains partial accuracy but includes major errors or significant omissions that affect its relevance to the reference.\",\n",
    "    \"score3_description\": \"The response is mostly accurate but lacks clarity, thoroughness, or minor details needed to fully address the reference.\",\n",
    "    \"score4_description\": \"The response is accurate and clear, with only minor omissions or slight inaccuracies in addressing the reference.\",\n",
    "    \"score5_description\": \"The response is completely accurate, clear, and thoroughly addresses the reference without any errors or omissions.\",\n",
    "}\n",
    "\n",
    "\n",
    "scorer = RubricsScore(rubrics=rubrics, llm=evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82e037",
   "metadata": {},
   "source": [
    "### Instance Specific rubrics criteria scoring\n",
    "\n",
    "- Instance Specific Evaluation Metric is a rubric-based method used to evaluate each item in a dataset individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da139f6e",
   "metadata": {},
   "source": [
    "This differs from the Rubric Based Criteria Scoring Metric, where a single rubric is applied to uniformly evaluate all items in the dataset. In the Instance-Specific Evaluation Metric, you decide which rubric to use for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1398670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import InstanceRubrics\n",
    "\n",
    "dataset = [\n",
    "    # Relevance to Query\n",
    "    {\n",
    "        \"user_query\": \"How do I handle exceptions in Python?\",\n",
    "        \"response\": \"To handle exceptions in Python, use the `try` and `except` blocks to catch and handle errors.\",\n",
    "        \"reference\": \"Proper error handling in Python involves using `try`, `except`, and optionally `else` and `finally` blocks to handle specific exceptions or perform cleanup tasks.\",\n",
    "        \"rubrics\": {\n",
    "            \"score0_description\": \"The response is off-topic or irrelevant to the user query.\",\n",
    "            \"score1_description\": \"The response is fully relevant and focused on the user query.\",\n",
    "        },\n",
    "    },\n",
    "    # Code Efficiency\n",
    "    {\n",
    "        \"user_query\": \"How can I create a list of squares for numbers 1 through 5 in Python?\",\n",
    "        \"response\": \"\"\"\n",
    "            # Using a for loop\n",
    "            squares = []\n",
    "            for i in range(1, 6):\n",
    "                squares.append(i ** 2)\n",
    "            print(squares)\n",
    "                \"\"\",\n",
    "        \"reference\": \"\"\"\n",
    "            # Using a list comprehension\n",
    "            squares = [i ** 2 for i in range(1, 6)]\n",
    "            print(squares)\n",
    "                \"\"\",\n",
    "        \"rubrics\": {\n",
    "            \"score0_description\": \"The code is inefficient and has obvious performance issues (e.g., unnecessary loops or redundant calculations).\",\n",
    "            \"score1_description\": \"The code is efficient, optimized, and performs well even with larger inputs.\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[InstanceRubrics(llm=evaluator_llm)],\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bd6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc773cf3",
   "metadata": {},
   "source": [
    "# Tasks Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f58a3e",
   "metadata": {},
   "source": [
    "### Summarization Score\n",
    "\n",
    "- SummarizationScore metric gives a measure of how well the summary (response) captures the important information from the retrieved_contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2935241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import SummarizationScore\n",
    "\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"A company is launching a fitness tracking app that helps users set exercise goals, log meals, and track water intake, with personalized workout suggestions and motivational reminders.\",\n",
    "    reference_contexts=[\n",
    "        \"A company is launching a new product, a smartphone app designed to help users track their fitness goals. The app allows users to set daily exercise targets, log their meals, and track their water intake. It also provides personalized workout recommendations and sends motivational reminders throughout the day.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer = SummarizationScore(llm=evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Complete_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
